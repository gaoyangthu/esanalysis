# consumer.conf: A single-node Flume configuration

# Name the components on this agent
consumer.sources = s
consumer.channels = c
consumer.sinks = k

# Describe/configure the source
consumer.sources.s.type = org.apache.flume.plugins.KafkaSource
consumer.sources.s.zookeeper.connect = 172.18.108.101:2181
consumer.sources.s.group.id = testGroup
consumer.sources.s.zookeeper.session.timeout.ms = 400
consumer.sources.s.zookeeper.sync.time.ms = 200
consumer.sources.s.auto.commit.interval.ms = 1000
consumer.sources.s.custom.topic.name = test
consumer.sources.s.custom.thread.per.consumer = 4

# Describe the sink
consumer.sinks.k.type = hdfs
consumer.sinks.k.hdfs.path = hdfs://bdcluster/esanalysis/nginx/log/
consumer.sinks.k.hdfs.writeFormat = Text
consumer.sinks.k.hdfs.fileType = DataStream
consumer.sinks.k.hdfs.rollInterval = 300
consumer.sinks.k.hdfs.rollSize = 60554432
consumer.sinks.k.hdfs.rollCount = 0
consumer.sinks.k.hdfs.batchSize = 1000
consumer.sinks.k.hdfs.txnEventMax = 1000
consumer.sinks.k.hdfs.callTimeout = 60000
consumer.sinks.k.hdfs.appendTimeout = 60000

# Use a channel which buffers events in memory
consumer.channels.c.type = memory
consumer.channels.c.capacity = 1000000
consumer.channels.c.transactionCapacity = 1000000

# Bind the source and sink to the channel
consumer.sources.s.channels = c
consumer.sinks.k.channel = c
